{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization.\n","metadata":{}},{"cell_type":"code","source":"text = \"Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization.  We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\"","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Built-in methods\n","metadata":{}},{"cell_type":"code","source":"tokens = text.split()\nprint(tokens[:5])","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization', 'is', 'one', 'of', 'the']\n"}]},{"cell_type":"markdown","source":"#### Sentence tokenization\n","metadata":{}},{"cell_type":"code","source":"tokens = text.split(\".\")\nprint(tokens[:3])","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization is one of the first step in any NLP pipeline', ' Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens', \" If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'\"]\n"}]},{"cell_type":"markdown","source":"### RegEx tokenization\n","metadata":{}},{"cell_type":"markdown","source":"#### Word tokenization\n","metadata":{}},{"cell_type":"code","source":"import re\n\ntokens = re.findall(\"[\\w]+\", text)\nprint(tokens[:5])","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization', 'is', 'one', 'of', 'the']\n"}]},{"cell_type":"markdown","source":"### NLTK library\n","metadata":{}},{"cell_type":"markdown","source":"#### Word tokenization\n","metadata":{}},{"cell_type":"code","source":"!pip install nltk","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ntokens = word_tokenize(text)\nprint(tokens[:5])","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization', 'is', 'one', 'of', 'the']\n"}]},{"cell_type":"markdown","source":"#### Sentence tokenization\n","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\ntokens = sent_tokenize(text)\nprint(tokens[:3])","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization is one of the first step in any NLP pipeline.', 'Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.', \"If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.\"]\n"}]},{"cell_type":"markdown","source":"### Spacy library\n","metadata":{}},{"cell_type":"markdown","source":"#### Word tokenization\n","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download en","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from spacy.lang.en import English\n\nenglish_tokenizer = English()\n\ndoc = english_tokenizer(text)\ntokens = [token.text for token in doc]\nprint(tokens[:5])","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"['Tokenization', 'is', 'one', 'of', 'the']\n"}]},{"cell_type":"markdown","source":"#### Sentence tokenization\n","metadata":{}},{"cell_type":"code","source":"english_tokenizer = English()\nenglish_tokenizer.add_pipe(\"sentencizer\")\n\n\ndoc = english_tokenizer(text)\ntokens = [token.sent for token in doc.sents]\nprint(tokens[:3])","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"[Tokenization is one of the first step in any NLP pipeline., Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens., If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.]\n"}]}]}
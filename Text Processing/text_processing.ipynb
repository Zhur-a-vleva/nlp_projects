{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text processing basics\n","metadata":{}},{"cell_type":"markdown","source":"## Sentence Segmentation\n\nSentence segmentation involves breaking down a text into individual sentences, typically separated by punctuation marks.\n","metadata":{}},{"cell_type":"code","source":"import nltk\n\ntext = \"This is a sample text. It contains multiple sentences. Can we segment it?\"\nsentences = nltk.sent_tokenize(text)\n\nprint(sentences)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']\n"}]},{"cell_type":"markdown","source":"## Lowercasing\n\nLowercasing converts all text to lowercase, ensuring uniformity and simplifying text processing.\n","metadata":{}},{"cell_type":"code","source":"text = \"ThIs Is AN ExaMple Text.\"\nlowercased_text = text.lower()\n\nprint(lowercased_text)","metadata":{},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"this is an example text.\n"}]},{"cell_type":"markdown","source":"## Stop Words Removal\n\nStop words are common words (e.g., \"the,\" \"and\") that are often removed during text processing to focus on meaningful words.\n","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nnltk.download(\"stopwords\", quiet=True)\n\ntext = \"This is an example sentence with some stop words.\"\nstop_words = set(stopwords.words(\"english\"))\n\nfiltered_words = [word for word in text.split() if word.lower() not in stop_words]\n\nprint(filtered_words)","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"['example', 'sentence', 'stop', 'words.']\n"}]},{"cell_type":"markdown","source":"## Lemmatization\n\nLemmatization reduces words to their base or dictionary form, considering the context and applying morphological analysis.\n","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nwords = [\"rocks\", \"corpora\", \"cries\"]\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n\nprint(lemmatized_words)","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"['rock', 'corpus', 'cry']\n"}]},{"cell_type":"markdown","source":"## Stemming\n\nStemming reduces words to their stems or root form, often by removing suffixes, in a more heuristic approach.\n","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nwords = [\"running\", \"rocks\", \"beautifully\"]\nstemmed_words = [stemmer.stem(word) for word in words]\n\nprint(stemmed_words)","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"['run', 'rock', 'beauti']\n"}]},{"cell_type":"markdown","source":"## Byte-Pair Encoding (BPE)\n\nBPE is a data compression technique used in NLP for tokenization. It breaks down words into subword units.\n","metadata":{}},{"cell_type":"code","source":"!pip install tokenizers","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tokenizers.processors import TemplateProcessing\n\nspecial_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\ntemp_proc = TemplateProcessing(\n    single=\"[CLS] $A [SEP]\",\n    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n    special_tokens=[\n        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n    ],\n)","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.models import BPE\nfrom tokenizers.decoders import BPEDecoder\n\ntokenizer = Tokenizer(BPE())\ntokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\ntokenizer.pre_tokenizer = Whitespace()\ntokenizer.decoder = BPEDecoder()\ntokenizer.post_processor = temp_proc","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tokenizers.trainers import BpeTrainer","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import gutenberg\n\nnltk.download(\"gutenberg\", quiet=True)\nnltk.download(\"punkt\", quiet=True)\n\ntrainer = BpeTrainer(vocab_size=5000, special_tokens=special_tokens)\nshakespeare = [\" \".join(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")]\ntokenizer.train_from_iterator(shakespeare, trainer=trainer)","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n\n\n\n"}]},{"cell_type":"code","source":"print(\n    tokenizer.encode(\n        \"BPE is a data compression technique used in NLP for tokenization.\"\n    ).tokens\n)\nprint(\n    tokenizer.encode(\n        \"Is this a danger which I see before me, the handle toward my hand?\"\n    ).tokens\n)","metadata":{},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"['[CLS]', 'b', 'pe', 'is', 'a', 'd', 'at', 'a', 'com', 'pre', 'ss', 'ion', 'te', 'ch', 'ni', 'que', 'use', 'd', 'in', 'n', 'lp', 'for', 'to', 'ken', 'iz', 'ation', '.', '[SEP]']\n\n['[CLS]', 'is', 'this', 'a', 'danger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'handle', 'toward', 'my', 'hand', '?', '[SEP]']\n"}]},{"cell_type":"markdown","source":"## Levenshtein edit distance\n\nEdit distance measures the similarity between two strings by counting the minimum number of operations needed to transform one string into the other.\n\n[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance#Example)\n","metadata":{}},{"cell_type":"code","source":"!pip install python-Levenshtein","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import Levenshtein\n\nword1 = \"kitten\"\nword2 = \"sitting\"\ndistance = Levenshtein.distance(word1, word2)\nprint(f\"Edit distance between '{word1}' and '{word2}': {distance}\")","metadata":{},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Edit distance between 'kitten' and 'sitting': 3\n"}]},{"cell_type":"markdown","source":"# Task\n","metadata":{}},{"cell_type":"markdown","source":"The aim of is to count the 10 most frequent words in the plays presented in the `data.txt` file.\n","metadata":{}},{"cell_type":"code","source":"with open(\"data.txt\") as f:\n    data = f.read()\nplays = data.split(\"\\n\")\nplays","metadata":{},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":["['austen-emma.txt',\n"," 'austen-persuasion.txt',\n"," 'austen-sense.txt',\n"," 'shakespeare-macbeth.txt',\n"," 'shakespeare-hamlet.txt',\n"," 'shakespeare-caesar.txt']"]},"metadata":{}}]},{"cell_type":"code","source":"plays_dict = {}\n\nfor play in plays:\n    plays_dict[play] = gutenberg.raw(play)\n    print(play, len(plays_dict[play]))","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"austen-emma.txt 887071\n\nausten-persuasion.txt 466292\n\nausten-sense.txt 673022\n\nshakespeare-macbeth.txt 100351\n\nshakespeare-hamlet.txt 162881\n\nshakespeare-caesar.txt 112310\n"}]},{"cell_type":"code","source":"def top_frequent_words(text, topk=10):\n    text = text.lower()\n\n    tokenizer = nltk.tokenize.RegexpTokenizer(\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    stop_words = stopwords.words(\"english\")\n    lemmatizer = WordNetLemmatizer()\n\n    words = [lemmatizer.lemmatize(token) for token in tokens]\n    words = [word for word in words if word not in stop_words]\n    freqs = nltk.FreqDist(words)\n    topk_common = freqs.most_common(topk)\n    return topk_common","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"top_words = {}\nfor play, text in plays_dict.items():\n    top_words[play] = top_frequent_words(text)","metadata":{},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with open(\"answer.csv\", \"w\") as f:\n    f.write(\"id,count\\n\")\n    for play, counts in top_words.items():\n        for i, count in enumerate(counts):\n            f.write(f\"{play}_{i},{count[1]}\\n\")","metadata":{},"execution_count":18,"outputs":[]}]}
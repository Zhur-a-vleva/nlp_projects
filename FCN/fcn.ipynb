{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70796,"databundleVersionId":7735149,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fully connected language models","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\ntext = \"\"\"\nAlice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: \nonce or twice she had peeped into the book her sister was reading, but it had no pictures or conversations \nin it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n\"\"\"\n\nclass CharLanguageModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CharLanguageModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input):\n        embedded = self.embedding(input)\n        output = self.fc(embedded)\n        output = self.softmax(output)\n        return output\n\n# Prepare training data\nall_characters = string.printable\nn_characters = len(all_characters)\n\n# Function to convert text to tensor\ndef text_to_tensor(text):\n    tensor = torch.zeros(len(text)).long()\n    for c in range(len(text)):\n        tensor[c] = all_characters.index(text[c])\n    return tensor\n\n# Function to generate training examples\ndef generate_training_example(text, chunk_len):\n    start_idx = torch.randint(0, len(text) - chunk_len, (1,))\n    end_idx = start_idx + chunk_len + 1\n    input_seq = text_to_tensor(text[start_idx:end_idx])\n    target_seq = text_to_tensor(text[start_idx+1:end_idx+1])\n    return input_seq, target_seq\n\n# Instantiate the model\nhidden_size = 100\nmodel = CharLanguageModel(n_characters, hidden_size, n_characters)\n\n# Loss function and optimizer\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nn_iters = 10000\nprint_every = 1000\nchunk_len = 10\nfor i in range(1, n_iters + 1):\n    input_seq, target_seq = generate_training_example(text, chunk_len)\n    if len(input_seq)!= len(target_seq):\n        continue\n    optimizer.zero_grad()\n    output = model(input_seq.unsqueeze(0))\n    loss = criterion(output.squeeze(0), target_seq)\n    loss.backward()\n    optimizer.step()\n\n    if i % print_every == 0:\n        print(f'Iteration {i}, Loss: {loss.item()}')\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:32:41.931770Z","iopub.status.busy":"2024-02-17T10:32:41.931415Z","iopub.status.idle":"2024-02-17T10:32:50.965834Z","shell.execute_reply":"2024-02-17T10:32:50.964892Z","shell.execute_reply.started":"2024-02-17T10:32:41.931745Z"}},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"Iteration 1000, Loss: 1.4660900831222534\n\nIteration 2000, Loss: 1.4820716381072998\n\nIteration 3000, Loss: 1.5302850008010864\n\nIteration 4000, Loss: 0.9148682951927185\n\nIteration 5000, Loss: 0.9483805298805237\n\nIteration 6000, Loss: 1.624928593635559\n\nIteration 7000, Loss: 1.2545435428619385\n\nIteration 8000, Loss: 1.5472545623779297\n\nIteration 9000, Loss: 1.2973827123641968\n\nIteration 10000, Loss: 1.3007453680038452\n"}]},{"cell_type":"code","source":"# Function to generate text using the trained model\ndef generate_text(start_string='Hello', predict_len=100, temperature=0.9):\n    model.eval()\n    input_seq = text_to_tensor(start_string)\n    hidden = torch.zeros(1, hidden_size)\n    output_str = start_string\n\n    for i in range(predict_len):\n        output = model(input_seq.unsqueeze(0))\n        output_dist = output.squeeze(0)[-1].div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        predicted_char = all_characters[top_i]\n        output_str += predicted_char\n        input_seq = text_to_tensor(predicted_char)\n    \n    return output_str\n\n# Generate text\ngenerated_text = generate_text(start_string='Alice', predict_len=50)\nprint(generated_text)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:35:00.869624Z","iopub.status.busy":"2024-02-17T10:35:00.869259Z","iopub.status.idle":"2024-02-17T10:35:00.889772Z","shell.execute_reply":"2024-02-17T10:35:00.887649Z","shell.execute_reply.started":"2024-02-17T10:35:00.869597Z"}},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Aliceg[vB|LR\u000b\nQ.LVW4[ -qTLc*B|AWZueV w(V@[!zhZ=t#8@ Ti<3\n"}]},{"cell_type":"markdown","source":"## Perplexity","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\nfrom collections import Counter\n\n# Sample text data\ntext = \"This is a sample text used for demonstration purposes. This is a text with two sentences.\"\n\n# Tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# Generate n-grams\nn = 3  # Choose the order of n-gram model\nn_grams = list(ngrams(tokens, n))\n\n# Count the occurrences of each n-gram\nn_gram_counts = Counter(n_grams)\n\nprint(\"Sample n-grams and their counts:\")\nfor n_gram, count in n_gram_counts.most_common(5):\n    print(n_gram, \"->\", count)\n\n# Calculate probabilities for each n-gram\ntotal_n_grams = sum(n_gram_counts.values())\nn_gram_probs = {n_gram: count / total_n_grams for n_gram, count in n_gram_counts.items()}\n\n# Define a function to predict the next word given a sequence of words\ndef predict_next_word(sequence):\n    next_word_probs = {}\n    for n_gram, prob in n_gram_probs.items():\n        if n_gram[:-1] == sequence:\n            next_word_probs[n_gram[-1]] = prob\n    return next_word_probs\n\n# Example usage:\nsequence = ('is', 'a')\nnext_word_probs = predict_next_word(sequence)\nprint(\"Next word probabilities:\", next_word_probs)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:46:15.536960Z","iopub.status.busy":"2024-02-17T10:46:15.536600Z","iopub.status.idle":"2024-02-17T10:46:15.547673Z","shell.execute_reply":"2024-02-17T10:46:15.546618Z","shell.execute_reply.started":"2024-02-17T10:46:15.536935Z"}},"execution_count":25,"outputs":[{"name":"stdout","output_type":"stream","text":"Sample n-grams and their counts:\n\n('This', 'is', 'a') -> 2\n\n('is', 'a', 'sample') -> 1\n\n('a', 'sample', 'text') -> 1\n\n('sample', 'text', 'used') -> 1\n\n('text', 'used', 'for') -> 1\n\nNext word probabilities: {'sample': 0.0625, 'text': 0.0625}\n"}]},{"cell_type":"code","source":"test_text = \"This is a test text.\"\n\n# Tokenize the test text\ntest_tokens = nltk.word_tokenize(test_text)\n\n# Generate n-grams for the test text\ntest_n_grams = list(ngrams(test_tokens, n))\n\n# Calculate perplexity\nperplexity = 1\nfor n_gram in test_n_grams:\n    if n_gram in n_gram_probs:\n        perplexity *= 1 / n_gram_probs[n_gram]\nperplexity = perplexity ** (1 / len(test_n_grams))\n\nprint(\"Perplexity of the language model:\", perplexity)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:36:58.702798Z","iopub.status.busy":"2024-02-17T10:36:58.702270Z","iopub.status.idle":"2024-02-17T10:36:58.711480Z","shell.execute_reply":"2024-02-17T10:36:58.709988Z","shell.execute_reply.started":"2024-02-17T10:36:58.702758Z"}},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"Perplexity of the language model: 1.681792830507429\n"}]},{"cell_type":"markdown","source":"## Beam search\n","metadata":{}},{"cell_type":"code","source":"def beam_search(seed_sequence, beam_width, max_length):\n    sequences = [[list(seed_sequence), 1.0]]\n    for _ in range(max_length):\n        next_sequences = []\n        for seq, score in sequences:\n            next_word_probs = predict_next_word(tuple(seq[-(n-1):]))\n            for next_word, next_word_prob in next_word_probs.items():\n                new_seq = seq + [next_word]\n                new_score = score * next_word_prob\n                next_sequences.append([new_seq, new_score])\n        next_sequences.sort(key=lambda x: x[1], reverse=True)\n        sequences = next_sequences[:beam_width]\n    return sequences\n\n# Example usage:\nseed_sequence = ['This', 'is']\ngenerated_sequences = beam_search(seed_sequence, beam_width=3, max_length=5)\nprint(\"Generated sequences:\")\nfor seq, score in generated_sequences:\n    print(\" \".join(seq), \"-> Score:\", score)\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:37:10.774227Z","iopub.status.busy":"2024-02-17T10:37:10.773871Z","iopub.status.idle":"2024-02-17T10:37:10.782862Z","shell.execute_reply":"2024-02-17T10:37:10.781682Z","shell.execute_reply.started":"2024-02-17T10:37:10.774198Z"}},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"Generated sequences:\n\nThis is a sample text used for -> Score: 3.0517578125e-05\n"}]},{"cell_type":"markdown","source":"## Task\n\nTask is to create fully connected N-gram based language model for text generation. ","metadata":{}},{"cell_type":"code","source":"!pip install torchtext","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-17T09:59:00.306568Z","iopub.status.busy":"2024-02-17T09:59:00.306219Z","iopub.status.idle":"2024-02-17T09:59:00.313185Z","shell.execute_reply":"2024-02-17T09:59:00.312203Z","shell.execute_reply.started":"2024-02-17T09:59:00.306539Z"}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Load data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.nn import functional as F\nfrom tqdm.notebook import tqdm\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\nMAX_TOKENS = 50000\n","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:00.328914Z","iopub.status.busy":"2024-02-17T09:59:00.328125Z","iopub.status.idle":"2024-02-17T09:59:06.833411Z","shell.execute_reply":"2024-02-17T09:59:06.832235Z","shell.execute_reply.started":"2024-02-17T09:59:00.328877Z"}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('train.csv')['text']\ntrain_data = train_data.fillna('')\ntrain_data.head()","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:06.843152Z","iopub.status.busy":"2024-02-17T09:59:06.842804Z","iopub.status.idle":"2024-02-17T09:59:07.589860Z","shell.execute_reply":"2024-02-17T09:59:07.588850Z","shell.execute_reply.started":"2024-02-17T09:59:06.843112Z"}},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":["0    they hope the president elect stands up to his...\n","1    it's the latest in a string of test launches o...\n","2    the need to control an undesirable situation b...\n","3                                                     \n","4    personal intelligence is new kid on the block ...\n","Name: text, dtype: object"]},"metadata":{}}]},{"cell_type":"markdown","source":"### Preprocessing\nAs the dataset is already preprocessed, we simply add `<start>` & `<end>` tokens to data","metadata":{}},{"cell_type":"code","source":"train_data = [\" \".join([\"<start>\", x, \"<end>\"]) for x in train_data]\ntrain_data[0]","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:07.591521Z","iopub.status.busy":"2024-02-17T09:59:07.591154Z","iopub.status.idle":"2024-02-17T09:59:07.702146Z","shell.execute_reply":"2024-02-17T09:59:07.700997Z","shell.execute_reply.started":"2024-02-17T09:59:07.591490Z"}},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["'<start> they hope the president elect stands up to his party <end>'"]},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_text(train_data, tokenizer):\n    for text in train_data:\n        tokens = tokenizer(text)\n        yield tokens","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:07.703680Z","iopub.status.busy":"2024-02-17T09:59:07.703368Z","iopub.status.idle":"2024-02-17T09:59:07.708155Z","shell.execute_reply":"2024-02-17T09:59:07.707261Z","shell.execute_reply.started":"2024-02-17T09:59:07.703653Z"}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer('basic_english')\nvocab = vocab = build_vocab_from_iterator(\n    tokenize_text(train_data, tokenizer),\n    specials=[\"<unk>\"],\n    max_tokens=MAX_TOKENS,\n)\nvocab.set_default_index(vocab[\"<unk>\"])","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:07.709673Z","iopub.status.busy":"2024-02-17T09:59:07.709367Z","iopub.status.idle":"2024-02-17T09:59:12.384713Z","shell.execute_reply":"2024-02-17T09:59:12.383856Z","shell.execute_reply.started":"2024-02-17T09:59:07.709643Z"}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"idx2symbol = vocab.get_itos()\nsymbol2idx = vocab.get_stoi()","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:12.385991Z","iopub.status.busy":"2024-02-17T09:59:12.385740Z","iopub.status.idle":"2024-02-17T09:59:12.425227Z","shell.execute_reply":"2024-02-17T09:59:12.424218Z","shell.execute_reply.started":"2024-02-17T09:59:12.385970Z"}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Transform data\n\nTasnsform data for the our model","metadata":{}},{"cell_type":"code","source":"NGRAM = 5\nSEQUENCE_LENGTH = NGRAM + 1 # n-gram + next word\n\ninput_sequences = []\nfor sequence in train_data:\n    token_list = vocab(tokenizer(sequence))\n    for i in range(SEQUENCE_LENGTH, len(token_list) + 1):\n        sequences = token_list[i-SEQUENCE_LENGTH:i]\n        input_sequences.append(sequences)\nprint(\"The number of training input sequence :{}\".format(len(input_sequences)))\ninput_sequences = np.array(input_sequences)","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:12.426900Z","iopub.status.busy":"2024-02-17T09:59:12.426560Z","iopub.status.idle":"2024-02-17T09:59:24.742634Z","shell.execute_reply":"2024-02-17T09:59:24.741683Z","shell.execute_reply.started":"2024-02-17T09:59:12.426873Z"}},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"The number of training input sequence :3376911\n"}]},{"cell_type":"code","source":"X, y = input_sequences[:,:-1], input_sequences[:,-1]","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:24.746267Z","iopub.status.busy":"2024-02-17T09:59:24.745969Z","iopub.status.idle":"2024-02-17T09:59:24.750564Z","shell.execute_reply":"2024-02-17T09:59:24.749647Z","shell.execute_reply.started":"2024-02-17T09:59:24.746242Z"}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Model\nDefine simple model for training","metadata":{}},{"cell_type":"code","source":"embedding_dim = 64\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FCNModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n        super().__init__()\n\n        self.embedding = nn.Embedding(\n            vocab_size,\n            embedding_dim,\n        )\n        self.hidden = nn.Linear(embedding_dim*(SEQUENCE_LENGTH - 1), hidden_dim)\n        self.classify = nn.Linear(hidden_dim, vocab_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, inputs):\n        outs = self.embedding(inputs)\n        outs = torch.flatten(outs, start_dim=1)\n        outs = self.hidden(outs)\n        outs = self.relu(outs)\n        logits = self.classify(outs)\n        return logits\n    \nmodel = FCNModel(vocab.__len__(), embedding_dim).to(device)","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:24.752222Z","iopub.status.busy":"2024-02-17T09:59:24.751899Z","iopub.status.idle":"2024-02-17T09:59:25.186022Z","shell.execute_reply":"2024-02-17T09:59:25.185129Z","shell.execute_reply.started":"2024-02-17T09:59:24.752193Z"}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"num_epochs = 1\n\ndataloader = DataLoader(\n    list(zip(y, X)),\n    batch_size=512,\n    shuffle=True,\n)","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:25.187373Z","iopub.status.busy":"2024-02-17T09:59:25.187087Z","iopub.status.idle":"2024-02-17T09:59:26.677033Z","shell.execute_reply":"2024-02-17T09:59:26.675925Z","shell.execute_reply.started":"2024-02-17T09:59:25.187349Z"}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\nfor epoch in range(num_epochs):\n    for labels, seqs in tqdm(dataloader):\n        # optimize\n        optimizer.zero_grad()\n        \n        logits = model(seqs.to(device))\n        \n        loss = F.cross_entropy(logits, labels.to(device))\n        loss.backward()\n        optimizer.step()\n        \n        # calculate accuracy\n        pred_labels = logits.argmax(dim=1)\n        num_correct = (pred_labels == labels.to(device)).float().sum()\n        accuracy = num_correct / len(labels)\n        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n    print(\"\")","metadata":{"execution":{"iopub.execute_input":"2024-02-17T09:59:26.679108Z","iopub.status.busy":"2024-02-17T09:59:26.678732Z","iopub.status.idle":"2024-02-17T10:01:12.977985Z","shell.execute_reply":"2024-02-17T10:01:12.977054Z","shell.execute_reply.started":"2024-02-17T09:59:26.679075Z"}},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f94614706b5b4bfcb2abdbee20aba793","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6596 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Epoch 1 - loss: 5.8558 - accuracy: 0.16975\n"}]},{"cell_type":"markdown","source":"### Predict","metadata":{}},{"cell_type":"code","source":"start_index = symbol2idx[\"<start>\"]\nend_index = symbol2idx[\"<end>\"]\nMAX_SENTENCE_LENGTH = 128\n\ndef predict(sentence, progressive_output=True):\n    test_sequence = vocab(tokenizer(sentence))\n    \n    if len(test_sequence) < NGRAM:\n        return [start_index, *test_sequence, end_index]\n    \n    test_sequence.insert(0, start_index)\n    \n    for _ in range(MAX_SENTENCE_LENGTH):\n        input_tensor = torch.tensor([test_sequence[-NGRAM:]], dtype=torch.int64).to(device)\n        pred_logits = model(input_tensor)\n        pred_index = pred_logits.argmax()\n        test_sequence.append(pred_index.item())\n        \n        if pred_index.item() == end_index:\n            break\n    return test_sequence\n\ndef decode_generated_sequence(generated_sequence):\n    return ' '.join(idx2symbol[symb] for symb in generated_sequence[1:-1])","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:01:12.979725Z","iopub.status.busy":"2024-02-17T10:01:12.979198Z","iopub.status.idle":"2024-02-17T10:01:12.987909Z","shell.execute_reply":"2024-02-17T10:01:12.987039Z","shell.execute_reply.started":"2024-02-17T10:01:12.979686Z"}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"generated_sequence = predict(\"health experts said it is\", progressive_output=False)\ndecode_generated_sequence(generated_sequence)","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:01:12.989445Z","iopub.status.busy":"2024-02-17T10:01:12.989109Z","iopub.status.idle":"2024-02-17T10:01:13.022046Z","shell.execute_reply":"2024-02-17T10:01:13.021127Z","shell.execute_reply.started":"2024-02-17T10:01:12.989415Z"}},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":["'health experts said it is a good thing to be a lot of the'"]},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.read_csv('test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:01:13.023900Z","iopub.status.busy":"2024-02-17T10:01:13.023234Z","iopub.status.idle":"2024-02-17T10:01:13.077846Z","shell.execute_reply":"2024-02-17T10:01:13.076980Z","shell.execute_reply.started":"2024-02-17T10:01:13.023867Z"}},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sample</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>what if in doing so we won't just</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>it should have been a glorious week for</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>a few protesters who refused to leave remained</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>scientists didn't know if humans played that game</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>here are five ways to get some beauty</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                             sample\n","0   0                  what if in doing so we won't just\n","1   1            it should have been a glorious week for\n","2   2     a few protesters who refused to leave remained\n","3   4  scientists didn't know if humans played that game\n","4   5              here are five ways to get some beauty"]},"metadata":{}}]},{"cell_type":"code","source":"test_df['prediction'] = test_df['sample'].apply(lambda x :decode_generated_sequence(predict(str(x), progressive_output=False)))","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:01:13.079347Z","iopub.status.busy":"2024-02-17T10:01:13.079047Z","iopub.status.idle":"2024-02-17T10:02:25.833316Z","shell.execute_reply":"2024-02-17T10:02:25.832320Z","shell.execute_reply.started":"2024-02-17T10:01:13.079321Z"}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.execute_input":"2024-02-17T10:02:25.834822Z","iopub.status.busy":"2024-02-17T10:02:25.834512Z","iopub.status.idle":"2024-02-17T10:02:25.844760Z","shell.execute_reply":"2024-02-17T10:02:25.843761Z","shell.execute_reply.started":"2024-02-17T10:02:25.834796Z"}},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sample</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>what if in doing so we won't just</td>\n","      <td>what if in doing so we won ' t just a lot of the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>it should have been a glorious week for</td>\n","      <td>it should have been a glorious week for the fi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>a few protesters who refused to leave remained</td>\n","      <td>a few protesters who refused to leave remained</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>scientists didn't know if humans played that game</td>\n","      <td>scientists didn ' t know if humans played that...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>here are five ways to get some beauty</td>\n","      <td>here are five ways to get some beauty of the w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                             sample  \\\n","0   0                  what if in doing so we won't just   \n","1   1            it should have been a glorious week for   \n","2   2     a few protesters who refused to leave remained   \n","3   4  scientists didn't know if humans played that game   \n","4   5              here are five ways to get some beauty   \n","\n","                                          prediction  \n","0   what if in doing so we won ' t just a lot of the  \n","1  it should have been a glorious week for the fi...  \n","2     a few protesters who refused to leave remained  \n","3  scientists didn ' t know if humans played that...  \n","4  here are five ways to get some beauty of the w...  "]},"metadata":{}}]}]}